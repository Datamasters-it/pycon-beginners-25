{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9527c560-6180-43b2-ba1e-54f047670e6a",
   "metadata": {},
   "source": [
    "# Costruisci un algoritmo di raccomandazione from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ddf4b1-6c20-4721-98ba-3796edbe5df7",
   "metadata": {},
   "source": [
    "## Parsing del dataset Netflix\n",
    "\n",
    "In questa fase, importiamo e interpretiamo il file CSV contenente i titoli di Netflix, strutturando ogni riga come un dizionario Python.\n",
    "\n",
    "L'uso di [csv.DictReader](https://docs.python.org/3/library/csv.html#csv.DictReader) consente di leggere direttamente i valori associandoli alle intestazioni delle colonne, facilitando così l'accesso ai campi tramite nomi simbolici.\n",
    "\n",
    "Durante il parsing:\n",
    "\n",
    "- I campi testuali come `cast` e `listed_in` vengono convertiti in liste.\n",
    "- I valori mancanti vengono gestiti e ripuliti con `.strip()` e controlli condizionali.\n",
    "- Il campo `release_year` viene convertito in intero solo se rappresenta un numero valido.\n",
    "\n",
    "Questa standardizzazione iniziale è essenziale per rendere i dati utilizzabili nei successivi step di analisi e raccomandazione.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1df5c6b-1dd3-4977-92db-0c72f70af0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0\n",
    "\n",
    "import csv\n",
    "import json \n",
    "\n",
    "# INPUT: path (string) - percorso del file CSV\n",
    "# OUTPUT: lista di dizionari, uno per ogni film/serie\n",
    "# SCOPO: carica e pulisce i dati da un file CSV Netflix\n",
    "def carica_dati_netflix(path):\n",
    "    dataset = []\n",
    "    return dataset\n",
    "\n",
    "# Esempio d'uso\n",
    "# dati = carica_dati_netflix('data/netflix_titles.csv')\n",
    "# print(json.dumps(dati[7659], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd46283-9331-417a-a25c-47a290d523c7",
   "metadata": {},
   "source": [
    "## Raccomandazioni per genere (filtraggio semplice)\n",
    "\n",
    "Queste due funzioni implementano una forma elementare di **sistema di raccomandazione basato su contenuto**, sfruttando il campo `listed_in` del dataset, che rappresenta i generi associati a ciascun titolo.\n",
    "\n",
    "### `raccomanda_per_genere`\n",
    "\n",
    "Filtra i titoli per singolo genere (es. \"Drama\"). Il confronto è case-insensitive e si basa su un'equivalenza diretta tra il genere richiesto e quelli associati al titolo. I risultati sono ordinati per anno di uscita in ordine decrescente.\n",
    "\n",
    "### `raccomanda_per_generi`\n",
    "\n",
    "Supporta la richiesta di più generi contemporaneamente (es. \"Drama, Thriller\"). L'algoritmo include tutti i titoli che contengono **almeno uno** dei generi richiesti.\n",
    "\n",
    "Queste funzioni non considerano ancora alcuna misura di similarità, distanza o vettorizzazione. Sono esempi di **filtraggio deterministico**, utile come baseline per confrontare approcci successivi più sofisticati.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7b35ec7-9657-4981-bfd2-ebf71db28623",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1\n",
    "\n",
    "# INPUT: \n",
    "#   dati (list of dict) - dataset dei titoli Netflix, ogni dict rappresenta un contenuto\n",
    "#   genere_input (str) - nome del genere da cercare (case-insensitive)\n",
    "# OUTPUT: \n",
    "#   None - la funzione stampa i risultati a schermo\n",
    "# SCOPO: \n",
    "#   stampa i titoli appartenenti al genere richiesto, ordinati per anno decrescente\n",
    "def raccomanda_per_genere(dati, genere_input):\n",
    "    pass\n",
    "\n",
    "# INPUT: \n",
    "#   dati (list of dict) - dataset dei titoli Netflix, ogni dict rappresenta un contenuto\n",
    "#   input_generi (str) - stringa con uno o più generi separati da virgole (es. \"Drama, Thriller\")\n",
    "# OUTPUT: \n",
    "#   None - la funzione stampa i risultati a schermo\n",
    "# SCOPO: \n",
    "#   stampa i titoli che appartengono ad almeno uno dei generi richiesti, ordinati per anno decrescente\n",
    "def raccomanda_per_generi(dati, input_generi):\n",
    "    pass\n",
    "\n",
    "# ESEMPIO USO\n",
    "# generi = input(\"Inserisci uno o più generi separati da virgola (es. Drama, Thriller): \")\n",
    "# raccomanda_per_generi(dati, generi)\n",
    "\n",
    "\n",
    "# ESEMPIO D'USO\n",
    "# genere = input(\"Inserisci un genere (es. Dramas, Thrillers): \")\n",
    "# raccomanda_per_genere(dati, genere)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4b6471-05b6-4138-baed-dd7101381b03",
   "metadata": {},
   "source": [
    "## Raccomandazione basata su similarità nel feature space\n",
    "\n",
    "A questo punto, passiamo da un filtraggio simbolico a una rappresentazione quantitativa. Ogni film viene proiettato in uno **spazio delle feature** definito dai generi presenti nel dataset.\n",
    "\n",
    "### One Hot Encoding binario\n",
    "\n",
    "Si costruisce un **vocabolario** di tutti i generi presenti (`costruisci_vocabolario_generi`) e ogni film viene rappresentato da un vettore binario (`vettore_generi`) della stessa lunghezza del vocabolario:\n",
    "\n",
    "- 1 se il film appartiene al genere in quella posizione\n",
    "- 0 altrimenti\n",
    "\n",
    "Questa codifica è un'applicazione diretta del **One Hot Encoding** per feature categoriali multivalore.\n",
    "\n",
    "### Similarità del coseno\n",
    "\n",
    "La **similarità tra vettori** è calcolata tramite la misura del coseno:\n",
    "\n",
    "$$\n",
    "\\text{sim}(\\vec{A}, \\vec{B}) = \\frac{\\vec{A} \\cdot \\vec{B}}{\\|\\vec{A}\\| \\cdot \\|\\vec{B}\\|}\n",
    "$$\n",
    "\n",
    "Dove:\n",
    "- $\\vec{A} \\cdot \\vec{B}$ è il prodotto scalare tra i due vettori\n",
    "- $\\|\\vec{A}\\|$ e $\\|\\vec{B}\\|$ sono le loro norme euclidee\n",
    "\n",
    "Questa metrica restituisce un valore tra 0 e 1 che indica quanto due vettori sono **orientati nella stessa direzione**. Non dipende dalla magnitudine, rendendola adatta per vettori binari.\n",
    "\n",
    "### Prossimità semantica\n",
    "\n",
    "Il concetto di **prossimità** viene dunque tradotto in termini geometrici: due film sono considerati simili se occupano posizioni vicine (in termini angolari) nello spazio generato dai generi.\n",
    "\n",
    "![Similarità del coseno tra vettori](asset/cosine-similarity-vectors.original.jpg)\n",
    "\n",
    "La funzione `raccomanda_simili` restituisce i top-N film più vicini al titolo di input secondo questa metrica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9627c2b3-e34d-40f0-98fc-b714ddd578b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# INPUT: \n",
    "#   dati (list of dict) - dataset dei titoli Netflix, ogni dict contiene una lista di generi in 'listed_in'\n",
    "# OUTPUT: \n",
    "#   list of str - vocabolario ordinato di tutti i generi unici presenti nel dataset (in lowercase)\n",
    "# SCOPO: \n",
    "#   costruire una lista di generi unici da usare come base per la vettorizzazione\n",
    "def costruisci_vocabolario_generi(dati):\n",
    "    pass\n",
    "\n",
    "# INPUT: \n",
    "#   record (dict) - un singolo titolo con campo 'listed_in' (lista di generi)\n",
    "#   vocabolario (list of str) - lista ordinata di tutti i generi possibili\n",
    "# OUTPUT: \n",
    "#   list of int - vettore binario che indica la presenza (1) o assenza (0) di ciascun genere\n",
    "# SCOPO: \n",
    "#   rappresentare i generi di un titolo come vettore binario basato sul vocabolario\n",
    "def vettore_generi(record, vocabolario):\n",
    "    pass\n",
    "\n",
    "# INPUT: \n",
    "#   v1 (list of float) - primo vettore numerico\n",
    "#   v2 (list of float) - secondo vettore numerico\n",
    "# OUTPUT: \n",
    "#   float - similarità del coseno tra i due vettori (valore tra 0 e 1)\n",
    "# SCOPO: \n",
    "#   calcolare quanto due vettori sono orientati nella stessa direzione nello spazio\n",
    "def similarita_coseno(v1, v2):\n",
    "    pass\n",
    "\n",
    "# INPUT: \n",
    "#   dati (list of dict) - dataset dei titoli Netflix\n",
    "#   titolo_input (str) - titolo da cercare (case-insensitive)\n",
    "# OUTPUT: \n",
    "#   dict or None - record corrispondente al titolo, oppure None se non trovato\n",
    "# SCOPO: \n",
    "#   trovare e restituire il dizionario di un titolo dato il nome, ignorando maiuscole/minuscole\n",
    "def trova_record_per_titolo(dati, titolo_input):\n",
    "    pass\n",
    "\n",
    "# INPUT: \n",
    "#   dati (list of dict) - dataset dei titoli Netflix\n",
    "#   titolo_input (str) - titolo da usare come riferimento (case-insensitive)\n",
    "#   top_n (int) - numero di titoli simili da restituire (default: 10)\n",
    "# OUTPUT: \n",
    "#   None - la funzione stampa i titoli simili a schermo\n",
    "# SCOPO: \n",
    "#   confrontare un titolo con tutti gli altri usando la similarità sui generi e stampare i top-N più simili\n",
    "def raccomanda_simili(dati, titolo_input, top_n=10):\n",
    "    pass\n",
    "\n",
    "# print(\"GENERI: \", costruisci_vocabolario_generi(dati), \"\\n\")\n",
    "# print(dati[7659]['title'], '->', dati[7659]['listed_in'])\n",
    "# print(\"VETTORE:\", vettore_generi(dati[7659], costruisci_vocabolario_generi(dati)), \"\\n\")\n",
    "# print(\"Cos Similarity: \", similarita_coseno([0, 1, 0], [1, 0, 0]), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c783826-316a-43ee-b7d4-4d7c7bd6963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# titolo = input(\"Inserisci il titolo di un film o serie: \")\n",
    "# raccomanda_simili(dati, titolo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d221314-e88a-48ab-b16b-d5a887fbcebd",
   "metadata": {},
   "source": [
    "## Espansione dello spazio vettoriale con feature categoriali multiple\n",
    "\n",
    "Finora lo spazio delle feature era limitato ai generi. In questa fase, estendiamo la rappresentazione vettoriale di ogni titolo includendo altre feature categoriali discrete:\n",
    "\n",
    "- `rating` (es. TV-MA, PG, etc.)\n",
    "- `type` (Movie o TV Show)\n",
    "- `director` (filtrando solo i **top-N più frequenti**)\n",
    "\n",
    "### Approccio\n",
    "\n",
    "- Le feature vengono one-hot encodate esattamente come fatto per i generi.\n",
    "- I valori di `rating` e `type` sono direttamente binarizzati.\n",
    "- Per `director`, si evita la codifica completa (troppo dispersiva) e si selezionano solo i più frequenti tramite [`Counter`](https://docs.python.org/3/library/collections.html#collections.Counter).\n",
    "\n",
    "La funzione `vettore_completo` concatena tutti i sottovettori binari in un unico vettore ad alta dimensionalità che rappresenta ogni contenuto in modo più ricco.\n",
    "\n",
    "### Similarità estesa\n",
    "\n",
    "La funzione `raccomanda_simili_esteso` sfrutta questa rappresentazione multidimensionale per calcolare la **similarità del coseno** su uno spazio vettoriale più articolato. Il sistema è ora in grado di cogliere affinità non solo tematiche (genere), ma anche stilistiche (regia), strutturali (tipo) e di target (rating).\n",
    "\n",
    "Questo rappresenta un primo passo concreto verso un sistema **ibrido basato su contenuto**, più sofisticato del semplice filtraggio su un attributo.\n",
    "\n",
    "## Uso di Counter\n",
    "\n",
    "```python\n",
    "from collections import Counter\n",
    "\n",
    "nomi = ['Luca', 'Anna', 'Luca', 'Marco', 'Anna', 'Luca']\n",
    "conteggio = Counter(nomi)\n",
    "print(conteggio)\n",
    "\n",
    "# output\n",
    "Counter({'Luca': 3, 'Anna': 2, 'Marco': 1})\n",
    "\n",
    "```\n",
    "\n",
    "Versione senza Counter\n",
    "\n",
    "```python\n",
    "director_counter = {}\n",
    "\n",
    "for r in dati:\n",
    "    if r['director']:\n",
    "        nome = r['director'].strip()\n",
    "        if nome not in director_counter:\n",
    "            director_counter[nome] = 1\n",
    "        else:\n",
    "            director_counter[nome] += 1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bdcf7d9-31c8-4761-8516-ac8fda6e831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# INPUT: \n",
    "#   dati (list of dict) - dataset dei titoli Netflix\n",
    "#   top_n_directors (int) - numero massimo di registi da includere (default: 20)\n",
    "# OUTPUT: \n",
    "#   tuple (list of str, list of str, list of str) - vocabolario ordinato dei rating, tipi e top-N registi più frequenti\n",
    "# SCOPO: \n",
    "#   estrarre i valori unici per rating e tipo, e i registi più frequenti per la codifica delle feature strutturate\n",
    "def costruisci_vocabolari_categorici(dati, top_n_directors=20):\n",
    "    pass\n",
    "\n",
    "\n",
    "# INPUT: \n",
    "#   record (dict) - un titolo con campi 'listed_in', 'rating', 'type', 'director'\n",
    "#   voc_generi (list of str) - vocabolario dei generi\n",
    "#   voc_rating (list of str) - vocabolario dei rating\n",
    "#   voc_type (list of str) - vocabolario dei tipi (Movie, TV Show)\n",
    "#   voc_directors (list of str) - lista dei registi più frequenti\n",
    "# OUTPUT: \n",
    "#   list of int - vettore binario concatenato che rappresenta tutte le feature strutturate del titolo\n",
    "# SCOPO: \n",
    "#   costruire la rappresentazione vettoriale completa di un titolo su base categoriale\n",
    "def vettore_completo(record, voc_generi, voc_rating, voc_type, voc_directors):\n",
    "    pass\n",
    "\n",
    "# INPUT: \n",
    "#   dati (list of dict) - dataset dei titoli Netflix\n",
    "#   titolo_input (str) - titolo da usare come riferimento (case-insensitive)\n",
    "#   top_n (int) - numero di titoli simili da stampare (default: 5)\n",
    "# OUTPUT: \n",
    "#   None - la funzione stampa i risultati a schermo\n",
    "# SCOPO: \n",
    "#   confrontare un titolo con tutti gli altri usando feature strutturate (generi, rating, tipo, regista) e stampare i top-N più simili\n",
    "def raccomanda_simili_esteso(dati, titolo_input, top_n=5):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad7a60ca-cf4a-4618-a52c-93dc7f7dbf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# titolo = input(\"Inserisci il titolo di un film o serie: \")\n",
    "# raccomanda_simili_esteso(dati, titolo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c47791-9eb7-4b59-a4ce-53efadb1706e",
   "metadata": {},
   "source": [
    "## Raccomandazione tramite analisi testuale: descrizioni e TF-IDF\n",
    "\n",
    "A differenza delle feature categoriali precedenti, il campo `description` è testuale e necessita di un trattamento diverso. Usiamo il modello **TF-IDF** (Term Frequency – Inverse Document Frequency) per rappresentare ogni descrizione come un vettore sparso che evidenzia i termini più rilevanti.\n",
    "\n",
    "### 1. Tokenizzazione e normalizzazione\n",
    "\n",
    "La funzione `tokenizza_testo` converte la descrizione in una lista di token, rimuovendo punteggiatura e portando tutto in minuscolo. Questo step è cruciale per costruire un vocabolario consistente.\n",
    "\n",
    "### 2. Term Frequency (TF)\n",
    "\n",
    "Per ogni film, viene calcolata la **frequenza relativa** di ciascuna parola rispetto al totale delle parole nella descrizione:\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{f_{t,d}}{\\sum_{t'} f_{t',d}}\n",
    "$$\n",
    "\n",
    "### 3. Inverse Document Frequency (IDF)\n",
    "\n",
    "L’IDF penalizza i termini troppo comuni nell’intero corpus, dando maggior peso a parole distintive:\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t) = \\log \\left( \\frac{N}{1 + \\text{df}(t)} \\right)\n",
    "$$\n",
    "\n",
    "dove:\n",
    "- $N$ è il numero totale di documenti\n",
    "- $\\text{df}(t)$ è il numero di documenti in cui il termine $t$ appare\n",
    "\n",
    "### 4. Vettori TF-IDF\n",
    "\n",
    "Ogni descrizione viene rappresentata come un dizionario sparso:\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\cdot \\text{IDF}(t)\n",
    "$$\n",
    "\n",
    "Questo consente di rappresentare contenuti con lunghezze diverse in uno spazio comune, evidenziando le parole semanticamente rilevanti.\n",
    "\n",
    "### 5. Similarità del coseno su vettori sparsi\n",
    "\n",
    "La similarità viene calcolata solo sulle parole in comune tra due descrizioni, evitando la costruzione esplicita di vettori densi di dimensione pari al vocabolario completo:\n",
    "\n",
    "$$\n",
    "\\cos(\\theta) = \\frac{\\sum_{i} A_i B_i}{\\|A\\| \\cdot \\|B\\|}\n",
    "$$\n",
    "\n",
    "### 6. Raccomandazione semantica\n",
    "\n",
    "`raccomanda_simili_descrizione` confronta la descrizione di un titolo con tutte le altre e restituisce i titoli semanticamente più vicini, sfruttando esclusivamente la rappresentazione testuale.\n",
    "\n",
    "Questo approccio consente di cogliere **affinità semantiche latenti** non esplicitamente codificate nelle feature strutturate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd852235-759e-4eb9-98bb-3c65125ddebd",
   "metadata": {},
   "source": [
    "## Esempio manuale su vocabolario ridotto\n",
    "\n",
    "Per chiarire il funzionamento del modello TF-IDF, consideriamo un sottoinsieme semplificato di tre descrizioni fittizie:\n",
    "\n",
    "1. \"A cat sits on the mat\"\n",
    "2. \"The dog lies on the mat\"\n",
    "3. \"A cat and a dog play together\"\n",
    "\n",
    "Supponiamo che il vocabolario risultante dalla tokenizzazione sia:\n",
    "\n",
    "```python\n",
    "[\"cat\", \"dog\", \"mat\", \"on\", \"the\", \"sits\", \"lies\", \"play\", \"together\"]\n",
    "```\n",
    "\n",
    "### 🔹 Calcolo TF\n",
    "\n",
    "Per la frase 1:\n",
    "\n",
    "Totale parole = 6  \n",
    "Parole: [\"a\", \"cat\", \"sits\", \"on\", \"the\", \"mat\"]\n",
    "\n",
    "Dopo rimozione di stopword come \"a\", rimangono:\n",
    "\n",
    "- TF(\"cat\") = 1/6\n",
    "- TF(\"sits\") = 1/6\n",
    "- TF(\"on\") = 1/6\n",
    "- TF(\"the\") = 1/6\n",
    "- TF(\"mat\") = 1/6\n",
    "\n",
    "### 🔹 Calcolo IDF\n",
    "\n",
    "Numero documenti $N = 3$\n",
    "\n",
    "| Termine   | Document Frequency (DF) | IDF                        |\n",
    "|-----------|--------------------------|----------------------------|\n",
    "| \"cat\"     | 2                        | $\\log(3 / (1 + 2)) = 0$    |\n",
    "| \"sits\"    | 1                        | $\\log(3 / 2) \\approx 0.405$|\n",
    "| \"mat\"     | 2                        | $\\log(3 / 3) = 0$          |\n",
    "| \"on\"      | 2                        | $\\log(3 / 3) = 0$          |\n",
    "| \"the\"     | 3                        | $\\log(3 / 4) < 0$          |\n",
    "\n",
    "### 🔹 Costruzione del vettore TF-IDF\n",
    "\n",
    "Moltiplichiamo TF per IDF per ciascun termine. I termini assenti non sono inclusi:\n",
    "\n",
    "| Termine   | TF    | IDF     | TF-IDF    |\n",
    "|-----------|-------|---------|-----------|\n",
    "| \"cat\"     | 1/6   | 0.000   | 0.000     |\n",
    "| \"sits\"    | 1/6   | 0.405   | 0.0675    |\n",
    "| \"mat\"     | 1/6   | 0.000   | 0.000     |\n",
    "| \"on\"      | 1/6   | 0.000   | 0.000     |\n",
    "| \"the\"     | 1/6   | < 0     | 0.000     |\n",
    "\n",
    "### 🔹 Vettore finale\n",
    "\n",
    "Nel vocabolario completo di 9 termini, la frase 1 sarà rappresentata così:\n",
    "\n",
    "**Forma densa (posizionale):**\n",
    "\n",
    "```python\n",
    "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0675, 0.0, 0.0, 0.0]\n",
    "```\n",
    "\n",
    "\n",
    "**Forma sparsa (più efficiente):**\n",
    "\n",
    "```python\n",
    "{\"sits\": 0.0675}\n",
    "```\n",
    "\n",
    "### Interpretazione\n",
    "\n",
    "Termini comuni e distribuiti su molti documenti, come \"cat\", \"mat\" e \"the\", ricevono un peso vicino a zero: il modello li riconosce come poco informativi. Al contrario, termini più rari e distintivi come \"sits\" ottengono un peso maggiore nel vettore TF-IDF.\n",
    "\n",
    "Questa rappresentazione enfatizza la specificità lessicale di ciascun documento, consentendo al sistema di raccomandazione di identificare affinità semantiche non evidenti nelle feature strutturate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "796108e6-d506-455d-905c-cbb4215c4fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT: \n",
    "#   testo (str) - stringa da tokenizzare\n",
    "# OUTPUT: \n",
    "#   list of str - lista di parole ottenute rimuovendo punteggiatura e convertendo in minuscolo\n",
    "# SCOPO: \n",
    "#   normalizzare e suddividere un testo in token puliti per analisi testuale\n",
    "def tokenizza_testo(testo):\n",
    "    pass\n",
    "\n",
    "# INPUT: \n",
    "#   dataset (list of dict) - ciascun dict rappresenta un titolo con una descrizione testuale\n",
    "# OUTPUT: \n",
    "#   list of dict - una lista in cui ogni elemento è un dizionario {parola: frequenza relativa} per una descrizione\n",
    "# SCOPO: \n",
    "#   calcolare la frequenza relativa (TF) delle parole in ciascuna descrizione del dataset\n",
    "def calcola_tf(dataset):\n",
    "    pass\n",
    "\n",
    "# INPUT: \n",
    "#   tf_per_film (list of dict) - lista di dizionari contenenti le TF per ciascun titolo\n",
    "# OUTPUT: \n",
    "#   dict - dizionario {parola: IDF}, con peso inverso alla frequenza nei documenti\n",
    "# SCOPO: \n",
    "#   calcolare l'inverse document frequency per ogni parola nel corpus\n",
    "def calcola_idf(tf_per_film):\n",
    "    pass\n",
    "\n",
    "# INPUT: \n",
    "#   tf_per_film (list of dict) - lista di TF per ciascuna descrizione\n",
    "#   idf (dict) - dizionario {parola: IDF} calcolato sull'intero corpus\n",
    "# OUTPUT: \n",
    "#   list of dict - lista di vettori TF-IDF sparsi per ciascun titolo\n",
    "# SCOPO: \n",
    "#   calcolare il vettore TF-IDF per ogni descrizione combinando TF locale e IDF globale\n",
    "def calcola_tfidf(tf_per_film, idf):\n",
    "    pass\n",
    "\n",
    "# INPUT: \n",
    "#   v1 (dict), v2 (dict) - due vettori TF-IDF sparsi {parola: peso}\n",
    "# OUTPUT: \n",
    "#   float - similarità del coseno tra i due vettori (valore tra 0 e 1)\n",
    "# SCOPO: \n",
    "#   calcolare la similarità del coseno tra vettori rappresentati come dizionari sparsi\n",
    "def similarita_coseno_sparsa(v1, v2):\n",
    "    pass\n",
    "\n",
    "# INPUT: \n",
    "#   dataset (list of dict) - dataset dei titoli Netflix\n",
    "#   vettori_tfidf (list of dict) - vettori TF-IDF sparsi associati alle descrizioni\n",
    "#   titolo_input (str) - titolo di riferimento (case-insensitive)\n",
    "#   top_n (int) - numero di titoli simili da mostrare (default: 5)\n",
    "# OUTPUT: \n",
    "#   None - la funzione stampa i risultati a schermo\n",
    "# SCOPO: \n",
    "#   trovare e stampare i top-N titoli con descrizioni più simili al titolo dato usando TF-IDF e similarità del coseno\n",
    "def raccomanda_simili_descrizione(dataset, vettori_tfidf, titolo_input, top_n=5):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3d5a850-ab53-421d-a16d-6c538b3d41d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf = calcola_tf(dati)\n",
    "# idf = calcola_idf(tf)\n",
    "# tfidf = calcola_tfidf(tf, idf)\n",
    "\n",
    "# print(dati[7659]['description'])\n",
    "# print(json.dumps(tf[7659], indent=4))\n",
    "# print(f\"Term Frequency for 'his' -> {tf[7659]['his']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8222cc84-ffba-4557-8ebb-68ff094c365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# titolo = input(\"Inserisci un titolo: \")\n",
    "# print(once['description'])\n",
    "# raccomanda_simili_descrizione(dati, tfidf, titolo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5385055-f382-47d3-a715-a52c675c3957",
   "metadata": {},
   "source": [
    "## Raccomandazione ibrida: contenuto testuale + strutturale\n",
    "\n",
    "Questa fase integra due rappresentazioni distinte di ciascun titolo:\n",
    "\n",
    "- Un **vettore sparso TF-IDF** basato sulla descrizione testuale\n",
    "- Un **vettore strutturato** binario costruito su generi, tipo, rating e regista\n",
    "\n",
    "### Fusione dei segnali: similarità pesata\n",
    "\n",
    "La funzione `similarita_pesata` calcola una **combinazione lineare** tra due misure di similarità del coseno:\n",
    "\n",
    "$$\n",
    "\\text{sim}_{\\text{ibrida}} = \\alpha \\cdot \\text{sim}_{\\text{TFIDF}} + (1 - \\alpha) \\cdot \\text{sim}_{\\text{struct}}\n",
    "$$\n",
    "\n",
    "dove:\n",
    "- $\\text{sim}_{\\text{TFIDF}}$ è la similarità del coseno tra descrizioni testuali\n",
    "- $\\text{sim}_{\\text{struct}}$ è la similarità del coseno tra vettori categoriali (generi, rating, ecc.)\n",
    "- $\\alpha$ è il **peso assegnato al contenuto testuale**, tipicamente $\\alpha = 0.7$\n",
    "\n",
    "### Algoritmo\n",
    "\n",
    "La funzione `raccomanda_ibrido`:\n",
    "\n",
    "1. Recupera sia la descrizione vettorializzata (`v_tfidf`) sia la rappresentazione strutturale (`v_struct`) del titolo richiesto\n",
    "2. Confronta queste due rappresentazioni con tutti gli altri titoli del dataset\n",
    "3. Calcola la similarità pesata\n",
    "4. Restituisce i titoli con la similarità ibrida più alta\n",
    "\n",
    "### Vantaggi del modello ibrido\n",
    "\n",
    "- **Complementarietà semantica**: le descrizioni testuali catturano significati che non emergono da generi o classificazioni.\n",
    "- **Robustezza strutturale**: i metadati strutturati compensano descrizioni brevi o poco informative.\n",
    "- Il parametro $\\alpha$ consente di **regolare dinamicamente** il bilanciamento tra le due fonti di informazione.\n",
    "\n",
    "Questo approccio è una forma semplificata ma efficace di **content-based hybrid recommendation**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47c68f44-01ea-4283-9f5c-5bb2a9514dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT: \n",
    "#   v_tfidf1, v_tfidf2 (dict) - vettori TF-IDF sparsi dei due titoli\n",
    "#   v_struct1, v_struct2 (list of int) - vettori strutturali binari dei due titoli\n",
    "#   peso_tfidf (float) - peso assegnato alla similarità TF-IDF (default: 0.7)\n",
    "# OUTPUT: \n",
    "#   float - similarità pesata combinando testo e struttura\n",
    "# SCOPO: \n",
    "#   calcolare una similarità ibrida tra due titoli combinando descrizione testuale e feature strutturate\n",
    "def similarita_pesata(v_tfidf1, v_struct1, v_tfidf2, v_struct2, peso_tfidf=0.7):\n",
    "    pass\n",
    "\n",
    "# INPUT: \n",
    "#   dati (list of dict) - dataset dei titoli Netflix\n",
    "#   vettori_tfidf (list of dict) - vettori TF-IDF sparsi associati alle descrizioni\n",
    "#   titolo_input (str) - titolo di riferimento (case-insensitive)\n",
    "#   peso_tfidf (float) - peso assegnato alla componente testuale (default: 0.7)\n",
    "#   top_n (int) - numero di titoli simili da mostrare (default: 5)\n",
    "# OUTPUT: \n",
    "#   None - la funzione stampa i risultati a schermo\n",
    "# SCOPO: \n",
    "#   raccomandare i top-N titoli più simili combinando descrizione testuale (TF-IDF) e metadati strutturali\n",
    "def raccomanda_ibrido(dati, vettori_tfidf, titolo_input, peso_tfidf=0.7, top_n=5):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c507a805-3641-45c2-a337-b239e960b91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# titolo = input(\"Inserisci il titolo di un film o serie: \")\n",
    "# raccomanda_ibrido(dati, tfidf, titolo, peso_tfidf=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904995e0-8dfe-4f4d-ad36-404750379992",
   "metadata": {},
   "source": [
    "## Raccomandazione semantica con Sentence Embeddings\n",
    "\n",
    "In questa fase il sistema utilizza rappresentazioni dense delle descrizioni testuali, generate da modelli di tipo Transformer addestrati per catturare la semantica delle frasi. Ogni descrizione viene convertita in un vettore numerico ad alta dimensionalità che riflette il significato latente del testo, non solo le parole che lo compongono.\n",
    "\n",
    "### Embedding semantici\n",
    "\n",
    "Utilizzando un modello pre-addestrato come \"all-distilroberta-v1\", ogni descrizione viene proiettata in uno spazio vettoriale in cui frasi simili (anche se lessicalmente diverse) risultano vicine. Questi modelli sono stati ottimizzati per produrre embedding confrontabili tramite misure geometriche come la similarità del coseno.\n",
    "\n",
    "A differenza del TF-IDF, che si basa su frequenze e presenze di termini, gli embeddings sono in grado di cogliere:\n",
    "\n",
    "- sinonimie e parafrasi\n",
    "- relazioni concettuali non esplicite\n",
    "- contesti sintattici\n",
    "\n",
    "Ad esempio, frasi come “Two kids are lost in space” e “A pair of children explore the galaxy” risultano semanticamente simili, pur non condividendo parole chiave.\n",
    "\n",
    "### Raccomandazione su base semantica\n",
    "\n",
    "Una volta ottenuti gli embedding delle descrizioni, si può confrontare qualsiasi titolo con tutti gli altri calcolando la similarità tra i relativi vettori. Questo approccio consente raccomandazioni basate sul significato del contenuto, indipendentemente dai generi o da altre etichette esplicite.\n",
    "\n",
    "### Modello ibrido: embedding + metadati\n",
    "\n",
    "La raccomandazione semantica può essere ulteriormente potenziata integrando anche la rappresentazione strutturale (generi, rating, tipo, regista). Si definisce così una similarità pesata che combina due contributi:\n",
    "\n",
    "- similarità semantica tra descrizioni\n",
    "- similarità strutturale basata su feature categoriali\n",
    "\n",
    "La combinazione avviene tramite una media pesata tra le due componenti, regolabile tramite un parametro che definisce l'importanza relativa del contenuto testuale rispetto alla struttura.\n",
    "\n",
    "Questo modello ibrido consente di sfruttare punti di forza complementari: la profondità semantica degli embedding e l’informazione esplicita contenuta nei metadati.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00811733-32d2-407c-824b-708e65016b60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from sentence-transformers) (0.31.2)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d51cf2e-3a85-488e-b0fc-c543b60e6404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pycon-beginners-25/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# modello = SentenceTransformer('all-MiniLM-L6-v2')  # Veloce e buono\n",
    "modello = SentenceTransformer('all-distilroberta-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "898e2273-ecbc-4919-acbe-01a497ead9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genera_embedding_descrizioni(dati, modello):\n",
    "    descrizioni = [r['description'] if r['description'] else '' for r in dati]\n",
    "    return modello.encode(descrizioni, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c166abc-a8a8-48c8-9d97-c18caad8b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def similarita_coseno_vec(v1, v2):\n",
    "    if norm(v1) == 0 or norm(v2) == 0:\n",
    "        return 0.0\n",
    "    return dot(v1, v2) / (norm(v1) * norm(v2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c8bd461-4e2a-4417-8878-e36c65d5e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raccomanda_embedding(dati, embeddings, titolo_input, top_n=5):\n",
    "    titolo_input = titolo_input.strip().lower()\n",
    "    index = next((i for i, r in enumerate(dati) if r['title'].strip().lower() == titolo_input), None)\n",
    "\n",
    "    if index is None:\n",
    "        print(\"Titolo non trovato.\")\n",
    "        return\n",
    "\n",
    "    v_input = embeddings[index]\n",
    "    risultati = []\n",
    "\n",
    "    for i, v in enumerate(embeddings):\n",
    "        if i == index:\n",
    "            continue\n",
    "        sim = similarita_coseno_vec(v_input, v)\n",
    "        risultati.append((dati[i]['title'], sim))\n",
    "\n",
    "    top = sorted(risultati, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "    print(f\"\\nTitoli simili a '{dati[index]['title']}' (con embedding semantico):\\n\")\n",
    "    for titolo, score in top:\n",
    "        print(f\"{titolo} — similarità: {round(score, 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb26d2d9-3ed0-4aab-91c9-063a36599808",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dati' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m embedding_descrizioni = genera_embedding_descrizioni(\u001b[43mdati\u001b[49m, modello)\n",
      "\u001b[31mNameError\u001b[39m: name 'dati' is not defined"
     ]
    }
   ],
   "source": [
    "embedding_descrizioni = genera_embedding_descrizioni(dati, modello)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82696b67-2008-49a9-8c08-6eff2b731e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_descrizioni[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8487a190-d638-411d-b5da-fdac6ff59443",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "titolo = input(\"Inserisci il titolo di un film o serie: \")\n",
    "raccomanda_embedding(dati, embedding_descrizioni, titolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e4ea0-96ec-455b-bcb7-8ad08eda22c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raccomanda_embedding_ibrido(dati, embedding_descrizioni, titolo_input, peso_emb=0.7, top_n=5):\n",
    "    voc_gen = costruisci_vocabolario_generi(dati)\n",
    "    voc_rat, voc_typ, voc_dir = costruisci_vocabolari_categorici(dati)\n",
    "\n",
    "    titolo_input = titolo_input.strip().lower()\n",
    "    index = next((i for i, r in enumerate(dati) if r['title'].strip().lower() == titolo_input), None)\n",
    "\n",
    "    if index is None:\n",
    "        print(\"Titolo non trovato.\")\n",
    "        return\n",
    "\n",
    "    v_emb_input = embedding_descrizioni[index]\n",
    "    v_struct_input = vettore_completo(dati[index], voc_gen, voc_rat, voc_typ, voc_dir)\n",
    "\n",
    "    risultati = []\n",
    "    for i, r in enumerate(dati):\n",
    "        if i == index:\n",
    "            continue\n",
    "\n",
    "        v_emb = embedding_descrizioni[i]\n",
    "        v_struct = vettore_completo(r, voc_gen, voc_rat, voc_typ, voc_dir)\n",
    "\n",
    "        sim_emb = similarita_coseno_vec(v_emb_input, v_emb)\n",
    "        sim_struct = similarita_coseno(v_struct_input, v_struct)\n",
    "\n",
    "        sim_finale = peso_emb * sim_emb + (1 - peso_emb) * sim_struct\n",
    "        risultati.append((r['title'], sim_finale))\n",
    "\n",
    "    top = sorted(risultati, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "    print(f\"\\nTitoli simili a '{dati[index]['title']}' (embedding + colonne):\\n\")\n",
    "    for titolo, score in top:\n",
    "        print(f\"{titolo} — similarità: {round(score, 2)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabafa0e-f8cb-47a8-b3ae-cc9956b9cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "titolo = input(\"Inserisci un titolo: \")\n",
    "raccomanda_embedding_ibrido(dati, embedding_descrizioni, titolo, peso_emb=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df88a5-fb79-4d3c-9048-ae77309ea696",
   "metadata": {},
   "source": [
    "## Ultimo step - Profilazione utente e raccomandazione personalizzata\n",
    "\n",
    "Fino ad ora la raccomandazione era centrata su un titolo fornito in input. Ora passiamo a un'impostazione più realistica: generare raccomandazioni **a partire da una lista di titoli preferiti** da parte dell’utente.\n",
    "\n",
    "### Costruzione del profilo utente\n",
    "\n",
    "Un profilo utente è rappresentato da un vettore che sintetizza le preferenze implicite contenute nei film preferiti. Questo avviene tramite:\n",
    "\n",
    "- **Media degli embedding testuali**: aggregazione semantica delle descrizioni dei titoli scelti\n",
    "- **Media dei vettori strutturali**: aggregazione binaria di generi, rating, tipo e registi associati\n",
    "\n",
    "Queste due componenti rappresentano rispettivamente il profilo **semantico** e **strutturale** dell’utente.\n",
    "\n",
    "### Raccomandazione ibrida basata sul profilo\n",
    "\n",
    "La funzione di raccomandazione confronta ciascun titolo del dataset con il profilo dell’utente:\n",
    "\n",
    "- La similarità viene calcolata sia nel **feature space semantico** (embedding) che nel **feature space strutturale**\n",
    "- Il punteggio finale è una media pesata delle due componenti, regolata da un parametro che bilancia l’influenza del contenuto testuale rispetto ai metadati\n",
    "\n",
    "Questo approccio permette di generare suggerimenti **personalizzati e diversificati**, andando oltre la semplice similarità con un singolo film.\n",
    "\n",
    "### Considerazioni\n",
    "\n",
    "- Il profilo è **dinamico**: ogni variazione nella lista di preferiti cambia la rappresentazione e, di conseguenza, l'intero set raccomandato.\n",
    "- L’uso della media vettoriale, seppur semplice, si dimostra efficace per catturare la “direzione generale” delle preferenze dell’utente nello spazio semantico.\n",
    "- È possibile estendere questo modello introducendo pesi sui film (es. valutazioni esplicite) o tecniche più avanzate di aggregazione (es. attention).\n",
    "\n",
    "Questo modulo conclude la transizione verso un **sistema di raccomandazione completo**, in grado di operare su input testuale libero e adattarsi al profilo dell’utente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5daa818-c4f7-4dc7-9c1b-67b3a952d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT: \n",
    "#   film_preferiti (list of str) - titoli scelti dall’utente\n",
    "#   dati (list of dict) - dataset dei titoli Netflix\n",
    "#   embedding_descrizioni (list of array) - vettori di embedding semantici per ciascun titolo\n",
    "# OUTPUT: \n",
    "#   array - vettore medio che rappresenta il profilo semantico dell’utente, oppure None se nessun titolo è trovato\n",
    "# SCOPO: \n",
    "#   costruire un vettore di profilo utente mediando gli embedding dei titoli preferiti\n",
    "def profilo_utente(film_preferiti, dati, embedding_descrizioni):\n",
    "    pass\n",
    "\n",
    "# INPUT: \n",
    "#   film_preferiti (list of str) - titoli scelti dall’utente\n",
    "#   dati (list of dict) - dataset dei titoli Netflix\n",
    "#   embedding_descrizioni (list of array) - embedding semantici per ciascun titolo\n",
    "#   voc_gen, voc_rat, voc_typ, voc_dir (list of str) - vocabolari strutturali per generi, rating, tipo e registi\n",
    "# OUTPUT: \n",
    "#   tuple (array, list of float) - profilo utente: media degli embedding e media dei vettori strutturali; (None, None) se nessun titolo è valido\n",
    "# SCOPO: \n",
    "#   costruire un profilo utente ibrido combinando rappresentazione semantica e strutturale dei titoli preferiti\n",
    "def profilo_utente_ibrido(film_preferiti, dati, embedding_descrizioni, voc_gen, voc_rat, voc_typ, voc_dir):\n",
    "    pass\n",
    "\n",
    "# INPUT: \n",
    "#   dati (list of dict) - dataset dei titoli Netflix\n",
    "#   embedding_descrizioni (list of array) - embedding semantici per ciascun titolo\n",
    "#   film_preferiti (list of str) - titoli scelti dall’utente\n",
    "#   peso_emb (float) - peso assegnato alla componente semantica (default: 0.7)\n",
    "#   top_n (int) - numero di titoli da raccomandare (default: 5)\n",
    "# OUTPUT: \n",
    "#   None - la funzione stampa i risultati a schermo\n",
    "# SCOPO: \n",
    "#   raccomandare i top-N titoli più affini al profilo utente, combinando embedding e feature strutturate\n",
    "def raccomanda_ibrido_utente(dati, embedding_descrizioni, film_preferiti, peso_emb=0.7, top_n=5):\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6589207-a3bc-4d1b-ba68-a02f63ed8741",
   "metadata": {},
   "outputs": [],
   "source": [
    "film_preferiti = [\n",
    "    \"Once upon a time in the west\",\n",
    "    \"Dances with Wolves\",\n",
    "    \"The Outlaw Josey Wales\"\n",
    "]\n",
    "\n",
    "# raccomanda_ibrido_utente(dati, embedding_descrizioni, film_preferiti, peso_emb=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78860fc3-ab3a-4351-be61-238bc2d15162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
